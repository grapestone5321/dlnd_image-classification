{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CIFAR-10 Dataset: 171MB [01:49, 1.56MB/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3adc9d7a20>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return np.array((x)/(255))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(range(10))\n",
    "    return lb.transform(x)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a bach of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape = [None, *image_shape], name = 'x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape = [None, n_classes], name = 'y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, name = 'keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    weight = tf.Variable(tf.random_normal([*conv_ksize, tf.to_int32(x_tensor.get_shape().as_list()[3]), conv_num_outputs], mean=0.0, stddev=0.1))\n",
    "    bias = tf.Variable (tf.zeros(conv_num_outputs))\n",
    "    conv_strides_list = [1, conv_strides[0], conv_strides[1], 1]\n",
    "    conv_layer = tf.nn.conv2d (tf.to_float(x_tensor), weight, strides = conv_strides_list, padding = 'SAME')\n",
    "    conv_layer = tf.nn.bias_add (conv_layer, bias)\n",
    "    conv_layer = tf.nn.relu (conv_layer)\n",
    "    pool_ksize_list = [1, pool_ksize[0], pool_ksize[1], 1]\n",
    "    pool_strides_list = [1, pool_strides[0], pool_strides[1], 1]\n",
    "    max_pool_layer = tf.nn.max_pool (conv_layer, pool_ksize_list, pool_strides_list, padding = 'SAME')\n",
    "    return max_pool_layer \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    shape = x_tensor.get_shape().as_list()\n",
    "    dim = np.prod(shape[1:])\n",
    "    x2 = tf.reshape(x_tensor, [-1, dim])\n",
    "    return x2\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    batch_size = x_tensor.get_shape().as_list()[1]\n",
    "    weight = tf.Variable (tf.random_normal([batch_size, num_outputs], mean=0.0, stddev=0.1))\n",
    "    bias = tf.Variable (tf.zeros(num_outputs))\n",
    "    fully_conv_layer = tf.add (tf.matmul (x_tensor, weight), bias)\n",
    "    fully_conv_layer = tf.nn.relu (fully_conv_layer)\n",
    "    return fully_conv_layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    batch_size = x_tensor.get_shape().as_list()[1]\n",
    "    weight = tf.Variable (tf.random_normal([batch_size, num_outputs], mean=0.0, stddev=0.1))\n",
    "    bias = tf.Variable (tf.zeros(num_outputs))\n",
    "    output_layer = tf.add (tf.matmul (x_tensor, weight), bias)\n",
    "    return output_layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    \n",
    "    x_tensor = x\n",
    "    conv_num_outputs1 = 32\n",
    "    conv_num_outputs2 = 64\n",
    "    conv_num_outputs3 = 128\n",
    "    conv_ksize = (4,4)\n",
    "    conv_strides = (2,2)\n",
    "    pool_ksize = (4,4)\n",
    "    pool_strides = (2,2)\n",
    "    num_outputs = 500\n",
    "    \n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv = conv2d_maxpool (x_tensor, conv_num_outputs1, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv = conv2d_maxpool (conv, conv_num_outputs2, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv = tf.nn.dropout (conv, tf.to_float(keep_prob))\n",
    "    conv = conv2d_maxpool (conv, conv_num_outputs3, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    flat = flatten(conv)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    fully = fully_conn(flat, num_outputs)\n",
    "    fully = fully_conn(fully, num_outputs)\n",
    "    fully = tf.nn.dropout (fully, tf.to_float(keep_prob))\n",
    "    fully = fully_conn(fully, 30)\n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    output_data = output(fully, 10)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return output_data\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, keep_prob: keep_probability})\n",
    "    pass\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = session.run(cost, feed_dict={\n",
    "        x: feature_batch,\n",
    "        y: label_batch,\n",
    "        keep_prob: 1.})\n",
    "    valid_acc = session.run(accuracy, feed_dict={\n",
    "        x: valid_features,\n",
    "        y: valid_labels,\n",
    "        keep_prob: 1.})\n",
    "    print('Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(loss, valid_acc))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 200\n",
    "batch_size = 1024\n",
    "keep_probability = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.2985 Validation Accuracy: 0.096000\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     2.3016 Validation Accuracy: 0.106200\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     2.2908 Validation Accuracy: 0.095400\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     2.2716 Validation Accuracy: 0.151400\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     2.2425 Validation Accuracy: 0.153600\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     2.1998 Validation Accuracy: 0.175800\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     2.0880 Validation Accuracy: 0.217600\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     2.0181 Validation Accuracy: 0.275600\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     1.9260 Validation Accuracy: 0.313800\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     1.8176 Validation Accuracy: 0.338800\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     1.7419 Validation Accuracy: 0.345200\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     1.6691 Validation Accuracy: 0.376200\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     1.5994 Validation Accuracy: 0.388200\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     1.5561 Validation Accuracy: 0.390800\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     1.5143 Validation Accuracy: 0.409000\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     1.5171 Validation Accuracy: 0.407000\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     1.4277 Validation Accuracy: 0.430400\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     1.3910 Validation Accuracy: 0.446400\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     1.3671 Validation Accuracy: 0.446000\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     1.3406 Validation Accuracy: 0.452200\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     1.3034 Validation Accuracy: 0.469400\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     1.2848 Validation Accuracy: 0.462800\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     1.2673 Validation Accuracy: 0.466800\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     1.2431 Validation Accuracy: 0.487400\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     1.1996 Validation Accuracy: 0.488800\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     1.1774 Validation Accuracy: 0.495000\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     1.1687 Validation Accuracy: 0.494800\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     1.1611 Validation Accuracy: 0.497800\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     1.1252 Validation Accuracy: 0.500200\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     1.1122 Validation Accuracy: 0.502600\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     1.1343 Validation Accuracy: 0.487600\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     1.0618 Validation Accuracy: 0.511000\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     1.0689 Validation Accuracy: 0.498000\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     1.0220 Validation Accuracy: 0.518800\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     1.0113 Validation Accuracy: 0.517400\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     0.9748 Validation Accuracy: 0.521800\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     0.9617 Validation Accuracy: 0.519200\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     0.9410 Validation Accuracy: 0.523800\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     0.9464 Validation Accuracy: 0.517600\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     0.9216 Validation Accuracy: 0.528600\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     0.9038 Validation Accuracy: 0.529400\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     0.9103 Validation Accuracy: 0.521400\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     0.8801 Validation Accuracy: 0.524200\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     0.8627 Validation Accuracy: 0.529600\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     0.8552 Validation Accuracy: 0.526200\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     0.8663 Validation Accuracy: 0.519000\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     0.8460 Validation Accuracy: 0.527000\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     0.8092 Validation Accuracy: 0.526800\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     0.7778 Validation Accuracy: 0.538800\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     0.7794 Validation Accuracy: 0.537400\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     0.7581 Validation Accuracy: 0.537200\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     0.7386 Validation Accuracy: 0.537200\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     0.7415 Validation Accuracy: 0.534800\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     0.7204 Validation Accuracy: 0.535200\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     0.7164 Validation Accuracy: 0.535000\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     0.6994 Validation Accuracy: 0.535200\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     0.7033 Validation Accuracy: 0.535600\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     0.6819 Validation Accuracy: 0.536600\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     0.6646 Validation Accuracy: 0.534600\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     0.6585 Validation Accuracy: 0.534800\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     0.6458 Validation Accuracy: 0.526400\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     0.6325 Validation Accuracy: 0.539200\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     0.6183 Validation Accuracy: 0.537600\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     0.6111 Validation Accuracy: 0.540400\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     0.6050 Validation Accuracy: 0.542400\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     0.5909 Validation Accuracy: 0.535600\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     0.5667 Validation Accuracy: 0.536600\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     0.5731 Validation Accuracy: 0.536200\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     0.5973 Validation Accuracy: 0.526400\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     0.5709 Validation Accuracy: 0.530800\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     0.5427 Validation Accuracy: 0.536400\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     0.5232 Validation Accuracy: 0.542000\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     0.5264 Validation Accuracy: 0.538000\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     0.5175 Validation Accuracy: 0.534800\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     0.5434 Validation Accuracy: 0.525600\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     0.5013 Validation Accuracy: 0.536800\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     0.5185 Validation Accuracy: 0.528000\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     0.5171 Validation Accuracy: 0.534000\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     0.4635 Validation Accuracy: 0.534400\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     0.4534 Validation Accuracy: 0.535600\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     0.4712 Validation Accuracy: 0.535000\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     0.4684 Validation Accuracy: 0.532800\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     0.4370 Validation Accuracy: 0.535600\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     0.4435 Validation Accuracy: 0.538400\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     0.4465 Validation Accuracy: 0.533000\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     0.4457 Validation Accuracy: 0.536400\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     0.4373 Validation Accuracy: 0.532000\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     0.4197 Validation Accuracy: 0.538200\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     0.4075 Validation Accuracy: 0.537000\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     0.4168 Validation Accuracy: 0.532600\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     0.4018 Validation Accuracy: 0.536600\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     0.3897 Validation Accuracy: 0.541800\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     0.4099 Validation Accuracy: 0.530400\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     0.3815 Validation Accuracy: 0.536200\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     0.3788 Validation Accuracy: 0.535400\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     0.3641 Validation Accuracy: 0.532400\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     0.3647 Validation Accuracy: 0.531800\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     0.3648 Validation Accuracy: 0.529600\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     0.3538 Validation Accuracy: 0.539000\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     0.3320 Validation Accuracy: 0.536200\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss:     0.3603 Validation Accuracy: 0.532400\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss:     0.3439 Validation Accuracy: 0.543400\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss:     0.3386 Validation Accuracy: 0.531600\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss:     0.3604 Validation Accuracy: 0.538200\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss:     0.3378 Validation Accuracy: 0.535400\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss:     0.3399 Validation Accuracy: 0.533200\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss:     0.3360 Validation Accuracy: 0.533000\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss:     0.3164 Validation Accuracy: 0.538200\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss:     0.3269 Validation Accuracy: 0.534000\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss:     0.2942 Validation Accuracy: 0.531600\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss:     0.3002 Validation Accuracy: 0.535800\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss:     0.2940 Validation Accuracy: 0.537600\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss:     0.2875 Validation Accuracy: 0.534200\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss:     0.2667 Validation Accuracy: 0.537200\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss:     0.2662 Validation Accuracy: 0.537600\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss:     0.2820 Validation Accuracy: 0.527800\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss:     0.2766 Validation Accuracy: 0.530800\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss:     0.2733 Validation Accuracy: 0.534000\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss:     0.2680 Validation Accuracy: 0.533600\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss:     0.2651 Validation Accuracy: 0.543000\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss:     0.2458 Validation Accuracy: 0.533400\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss:     0.2522 Validation Accuracy: 0.532200\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss:     0.2360 Validation Accuracy: 0.534400\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss:     0.2388 Validation Accuracy: 0.533800\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss:     0.2239 Validation Accuracy: 0.536400\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss:     0.2208 Validation Accuracy: 0.533600\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss:     0.2175 Validation Accuracy: 0.539000\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss:     0.2213 Validation Accuracy: 0.537000\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss:     0.2164 Validation Accuracy: 0.537800\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss:     0.2233 Validation Accuracy: 0.534800\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss:     0.2008 Validation Accuracy: 0.535400\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss:     0.1995 Validation Accuracy: 0.531000\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss:     0.2183 Validation Accuracy: 0.532200\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss:     0.2012 Validation Accuracy: 0.535600\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss:     0.1969 Validation Accuracy: 0.545000\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss:     0.1954 Validation Accuracy: 0.532200\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss:     0.1945 Validation Accuracy: 0.538000\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss:     0.1897 Validation Accuracy: 0.530400\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss:     0.1952 Validation Accuracy: 0.536600\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss:     0.1726 Validation Accuracy: 0.539400\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss:     0.1812 Validation Accuracy: 0.539200\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss:     0.1688 Validation Accuracy: 0.530000\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss:     0.1623 Validation Accuracy: 0.537200\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss:     0.1656 Validation Accuracy: 0.531400\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss:     0.1561 Validation Accuracy: 0.537200\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss:     0.1726 Validation Accuracy: 0.527400\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss:     0.1668 Validation Accuracy: 0.536800\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss:     0.1816 Validation Accuracy: 0.526800\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss:     0.1513 Validation Accuracy: 0.539400\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss:     0.1393 Validation Accuracy: 0.532800\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss:     0.1474 Validation Accuracy: 0.534800\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss:     0.1481 Validation Accuracy: 0.529000\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss:     0.1632 Validation Accuracy: 0.528200\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss:     0.1487 Validation Accuracy: 0.521400\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss:     0.1480 Validation Accuracy: 0.527800\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss:     0.1388 Validation Accuracy: 0.534600\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss:     0.1477 Validation Accuracy: 0.531400\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss:     0.1474 Validation Accuracy: 0.523200\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss:     0.1386 Validation Accuracy: 0.528800\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss:     0.1352 Validation Accuracy: 0.525800\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss:     0.1315 Validation Accuracy: 0.536000\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss:     0.1365 Validation Accuracy: 0.531000\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss:     0.1326 Validation Accuracy: 0.527200\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss:     0.1513 Validation Accuracy: 0.521600\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss:     0.1672 Validation Accuracy: 0.518600\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss:     0.1757 Validation Accuracy: 0.519800\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss:     0.1806 Validation Accuracy: 0.514800\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss:     0.1773 Validation Accuracy: 0.523400\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss:     0.1816 Validation Accuracy: 0.516200\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss:     0.1544 Validation Accuracy: 0.536200\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss:     0.1361 Validation Accuracy: 0.525000\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss:     0.1340 Validation Accuracy: 0.527000\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss:     0.1301 Validation Accuracy: 0.526600\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss:     0.1312 Validation Accuracy: 0.534000\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss:     0.1379 Validation Accuracy: 0.533600\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss:     0.1497 Validation Accuracy: 0.528600\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss:     0.1277 Validation Accuracy: 0.535000\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss:     0.1189 Validation Accuracy: 0.529600\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss:     0.1065 Validation Accuracy: 0.540600\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss:     0.1130 Validation Accuracy: 0.536600\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss:     0.0971 Validation Accuracy: 0.536200\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss:     0.0936 Validation Accuracy: 0.531200\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss:     0.0912 Validation Accuracy: 0.531600\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss:     0.0821 Validation Accuracy: 0.532400\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss:     0.0784 Validation Accuracy: 0.529400\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss:     0.0925 Validation Accuracy: 0.529400\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss:     0.0867 Validation Accuracy: 0.538400\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss:     0.0977 Validation Accuracy: 0.532600\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss:     0.1014 Validation Accuracy: 0.532600\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss:     0.0985 Validation Accuracy: 0.531400\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss:     0.0969 Validation Accuracy: 0.526800\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss:     0.1057 Validation Accuracy: 0.520000\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss:     0.0976 Validation Accuracy: 0.520400\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss:     0.0953 Validation Accuracy: 0.525600\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss:     0.0960 Validation Accuracy: 0.527800\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss:     0.0827 Validation Accuracy: 0.525800\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss:     0.0926 Validation Accuracy: 0.523200\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss:     0.0786 Validation Accuracy: 0.528800\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss:     0.0800 Validation Accuracy: 0.534800\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss:     0.0761 Validation Accuracy: 0.531600\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.2988 Validation Accuracy: 0.103000\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:     2.3010 Validation Accuracy: 0.111200\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:     2.2954 Validation Accuracy: 0.151200\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:     2.2564 Validation Accuracy: 0.169000\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:     2.2015 Validation Accuracy: 0.171800\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     2.1049 Validation Accuracy: 0.198200\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:     2.0715 Validation Accuracy: 0.212400\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:     1.9491 Validation Accuracy: 0.263800\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:     1.8301 Validation Accuracy: 0.292800\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:     1.7955 Validation Accuracy: 0.296200\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     1.7498 Validation Accuracy: 0.332600\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:     1.7331 Validation Accuracy: 0.330200\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:     1.6713 Validation Accuracy: 0.347000\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:     1.6560 Validation Accuracy: 0.356400\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:     1.6488 Validation Accuracy: 0.371400\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     1.5638 Validation Accuracy: 0.390400\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:     1.6015 Validation Accuracy: 0.403000\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:     1.5217 Validation Accuracy: 0.409000\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:     1.5255 Validation Accuracy: 0.427400\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:     1.5192 Validation Accuracy: 0.439800\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     1.4576 Validation Accuracy: 0.446000\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:     1.4798 Validation Accuracy: 0.455400\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:     1.4230 Validation Accuracy: 0.455600\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:     1.4336 Validation Accuracy: 0.455400\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:     1.3977 Validation Accuracy: 0.472600\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     1.3650 Validation Accuracy: 0.478200\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:     1.4049 Validation Accuracy: 0.488400\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:     1.3379 Validation Accuracy: 0.491800\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:     1.3442 Validation Accuracy: 0.485000\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:     1.3188 Validation Accuracy: 0.492200\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     1.3039 Validation Accuracy: 0.502000\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:     1.3444 Validation Accuracy: 0.504400\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:     1.2853 Validation Accuracy: 0.501800\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:     1.2859 Validation Accuracy: 0.512200\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:     1.2443 Validation Accuracy: 0.513800\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     1.2584 Validation Accuracy: 0.515800\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:     1.2977 Validation Accuracy: 0.519000\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:     1.2413 Validation Accuracy: 0.521000\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:     1.2568 Validation Accuracy: 0.516800\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:     1.1997 Validation Accuracy: 0.520200\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     1.2257 Validation Accuracy: 0.527800\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:     1.2479 Validation Accuracy: 0.519000\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:     1.1973 Validation Accuracy: 0.533000\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:     1.2156 Validation Accuracy: 0.527400\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:     1.1516 Validation Accuracy: 0.537400\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     1.2074 Validation Accuracy: 0.541800\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:     1.2159 Validation Accuracy: 0.538200\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:     1.1773 Validation Accuracy: 0.537000\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:     1.1775 Validation Accuracy: 0.535000\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:     1.1196 Validation Accuracy: 0.545800\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     1.1640 Validation Accuracy: 0.551000\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss:     1.1867 Validation Accuracy: 0.552200\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss:     1.1467 Validation Accuracy: 0.543800\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss:     1.1429 Validation Accuracy: 0.547600\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss:     1.0910 Validation Accuracy: 0.558800\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     1.1298 Validation Accuracy: 0.568400\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss:     1.1603 Validation Accuracy: 0.557600\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss:     1.0986 Validation Accuracy: 0.559200\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss:     1.1030 Validation Accuracy: 0.560000\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss:     1.0710 Validation Accuracy: 0.560600\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     1.1122 Validation Accuracy: 0.572600\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss:     1.1289 Validation Accuracy: 0.566400\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss:     1.0945 Validation Accuracy: 0.566200\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss:     1.0978 Validation Accuracy: 0.561800\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss:     1.0397 Validation Accuracy: 0.568800\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     1.0965 Validation Accuracy: 0.571400\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss:     1.1179 Validation Accuracy: 0.571800\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss:     1.0684 Validation Accuracy: 0.567200\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss:     1.0640 Validation Accuracy: 0.568400\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss:     1.0119 Validation Accuracy: 0.576800\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     1.1013 Validation Accuracy: 0.570200\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss:     1.0930 Validation Accuracy: 0.579200\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss:     1.0477 Validation Accuracy: 0.571800\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss:     1.0618 Validation Accuracy: 0.570800\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss:     0.9980 Validation Accuracy: 0.582200\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     1.0742 Validation Accuracy: 0.577800\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss:     1.0697 Validation Accuracy: 0.588600\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss:     1.0139 Validation Accuracy: 0.578600\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss:     1.0206 Validation Accuracy: 0.587600\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss:     0.9827 Validation Accuracy: 0.588600\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     1.0354 Validation Accuracy: 0.592400\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss:     1.0393 Validation Accuracy: 0.582400\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss:     0.9842 Validation Accuracy: 0.588400\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss:     1.0053 Validation Accuracy: 0.589800\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss:     0.9603 Validation Accuracy: 0.585000\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     1.0182 Validation Accuracy: 0.593600\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss:     1.0413 Validation Accuracy: 0.588600\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss:     0.9784 Validation Accuracy: 0.585400\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss:     0.9880 Validation Accuracy: 0.589200\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss:     0.9419 Validation Accuracy: 0.595000\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     0.9980 Validation Accuracy: 0.592400\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss:     1.0242 Validation Accuracy: 0.586200\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss:     0.9747 Validation Accuracy: 0.590400\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss:     0.9812 Validation Accuracy: 0.590400\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss:     0.9458 Validation Accuracy: 0.586600\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     0.9940 Validation Accuracy: 0.595800\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss:     0.9932 Validation Accuracy: 0.595600\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss:     0.9299 Validation Accuracy: 0.602400\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss:     0.9625 Validation Accuracy: 0.593000\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss:     0.9131 Validation Accuracy: 0.600000\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     0.9687 Validation Accuracy: 0.605600\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss:     0.9812 Validation Accuracy: 0.594400\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss:     0.9219 Validation Accuracy: 0.605000\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss:     0.9385 Validation Accuracy: 0.604400\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss:     0.9060 Validation Accuracy: 0.602200\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     0.9511 Validation Accuracy: 0.609600\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss:     0.9728 Validation Accuracy: 0.600000\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss:     0.9196 Validation Accuracy: 0.604400\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss:     0.9464 Validation Accuracy: 0.598400\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss:     0.9025 Validation Accuracy: 0.604600\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     0.9644 Validation Accuracy: 0.603800\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss:     0.9421 Validation Accuracy: 0.618600\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss:     0.8845 Validation Accuracy: 0.616600\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss:     0.9091 Validation Accuracy: 0.602200\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss:     0.8749 Validation Accuracy: 0.610000\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     0.9234 Validation Accuracy: 0.614200\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss:     0.9361 Validation Accuracy: 0.623600\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss:     0.8797 Validation Accuracy: 0.618800\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss:     0.9002 Validation Accuracy: 0.603600\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss:     0.8763 Validation Accuracy: 0.614200\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     0.9220 Validation Accuracy: 0.613800\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss:     0.9239 Validation Accuracy: 0.616400\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss:     0.8664 Validation Accuracy: 0.615200\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss:     0.8869 Validation Accuracy: 0.613200\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss:     0.8390 Validation Accuracy: 0.611600\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     0.8930 Validation Accuracy: 0.613400\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss:     0.9073 Validation Accuracy: 0.620400\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss:     0.8527 Validation Accuracy: 0.622600\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss:     0.8849 Validation Accuracy: 0.608400\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss:     0.8420 Validation Accuracy: 0.618400\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     0.8974 Validation Accuracy: 0.616400\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss:     0.8801 Validation Accuracy: 0.620600\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss:     0.8498 Validation Accuracy: 0.624200\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss:     0.8659 Validation Accuracy: 0.613800\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss:     0.8343 Validation Accuracy: 0.623000\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     0.8865 Validation Accuracy: 0.621800\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss:     0.8746 Validation Accuracy: 0.630000\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss:     0.8238 Validation Accuracy: 0.624400\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss:     0.8667 Validation Accuracy: 0.608400\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss:     0.8227 Validation Accuracy: 0.621600\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     0.8596 Validation Accuracy: 0.626200\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss:     0.8540 Validation Accuracy: 0.626000\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss:     0.8074 Validation Accuracy: 0.630600\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss:     0.8426 Validation Accuracy: 0.630200\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss:     0.7972 Validation Accuracy: 0.628600\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     0.8568 Validation Accuracy: 0.628800\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss:     0.8591 Validation Accuracy: 0.628000\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss:     0.8075 Validation Accuracy: 0.632200\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss:     0.8424 Validation Accuracy: 0.616000\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss:     0.7904 Validation Accuracy: 0.629400\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     0.8299 Validation Accuracy: 0.632000\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss:     0.8270 Validation Accuracy: 0.630600\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss:     0.7889 Validation Accuracy: 0.631400\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss:     0.8311 Validation Accuracy: 0.623600\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss:     0.7797 Validation Accuracy: 0.628200\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     0.8266 Validation Accuracy: 0.632600\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss:     0.8174 Validation Accuracy: 0.639200\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss:     0.7948 Validation Accuracy: 0.631000\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss:     0.8134 Validation Accuracy: 0.621400\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss:     0.7603 Validation Accuracy: 0.627800\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     0.8331 Validation Accuracy: 0.634200\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss:     0.8160 Validation Accuracy: 0.632000\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss:     0.7630 Validation Accuracy: 0.641600\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss:     0.7973 Validation Accuracy: 0.636800\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss:     0.7637 Validation Accuracy: 0.630000\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     0.8066 Validation Accuracy: 0.633600\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss:     0.8068 Validation Accuracy: 0.636200\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss:     0.7663 Validation Accuracy: 0.636600\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss:     0.8041 Validation Accuracy: 0.630400\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss:     0.7634 Validation Accuracy: 0.642000\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     0.7965 Validation Accuracy: 0.638600\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss:     0.7939 Validation Accuracy: 0.639400\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss:     0.7728 Validation Accuracy: 0.638400\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss:     0.7793 Validation Accuracy: 0.631400\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss:     0.7479 Validation Accuracy: 0.640400\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     0.7881 Validation Accuracy: 0.638200\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss:     0.7916 Validation Accuracy: 0.629400\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss:     0.7582 Validation Accuracy: 0.636600\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss:     0.7955 Validation Accuracy: 0.626400\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss:     0.7403 Validation Accuracy: 0.637400\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     0.7861 Validation Accuracy: 0.633600\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss:     0.7879 Validation Accuracy: 0.637000\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss:     0.7383 Validation Accuracy: 0.642200\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss:     0.7685 Validation Accuracy: 0.636400\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss:     0.7304 Validation Accuracy: 0.637400\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     0.7981 Validation Accuracy: 0.634800\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss:     0.7706 Validation Accuracy: 0.632200\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss:     0.7196 Validation Accuracy: 0.641000\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss:     0.7905 Validation Accuracy: 0.624000\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss:     0.7063 Validation Accuracy: 0.642000\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     0.7737 Validation Accuracy: 0.637400\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss:     0.7547 Validation Accuracy: 0.637800\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss:     0.7355 Validation Accuracy: 0.643400\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss:     0.7648 Validation Accuracy: 0.636600\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss:     0.7092 Validation Accuracy: 0.645400\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     0.7615 Validation Accuracy: 0.644000\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss:     0.7305 Validation Accuracy: 0.641000\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss:     0.7129 Validation Accuracy: 0.643000\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss:     0.7499 Validation Accuracy: 0.636400\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss:     0.6870 Validation Accuracy: 0.644000\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     0.7540 Validation Accuracy: 0.649600\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss:     0.7278 Validation Accuracy: 0.639800\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss:     0.6949 Validation Accuracy: 0.644400\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss:     0.7398 Validation Accuracy: 0.645000\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss:     0.6734 Validation Accuracy: 0.647400\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     0.7447 Validation Accuracy: 0.648000\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss:     0.7311 Validation Accuracy: 0.644000\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss:     0.7115 Validation Accuracy: 0.643400\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss:     0.7288 Validation Accuracy: 0.640000\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss:     0.6730 Validation Accuracy: 0.651000\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     0.7237 Validation Accuracy: 0.649200\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss:     0.7188 Validation Accuracy: 0.640400\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss:     0.6997 Validation Accuracy: 0.648000\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss:     0.7323 Validation Accuracy: 0.638400\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss:     0.6797 Validation Accuracy: 0.641400\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     0.7322 Validation Accuracy: 0.645200\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss:     0.7026 Validation Accuracy: 0.643000\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss:     0.7187 Validation Accuracy: 0.645800\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss:     0.7217 Validation Accuracy: 0.644800\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss:     0.6732 Validation Accuracy: 0.645000\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     0.7307 Validation Accuracy: 0.655600\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss:     0.7114 Validation Accuracy: 0.642800\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss:     0.6874 Validation Accuracy: 0.654400\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss:     0.7044 Validation Accuracy: 0.648200\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss:     0.6623 Validation Accuracy: 0.643000\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     0.7141 Validation Accuracy: 0.650200\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss:     0.6886 Validation Accuracy: 0.641200\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss:     0.6703 Validation Accuracy: 0.647200\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss:     0.7069 Validation Accuracy: 0.647000\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss:     0.6344 Validation Accuracy: 0.645000\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     0.7114 Validation Accuracy: 0.646600\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss:     0.6825 Validation Accuracy: 0.645400\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss:     0.6755 Validation Accuracy: 0.645400\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss:     0.6975 Validation Accuracy: 0.647800\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss:     0.6447 Validation Accuracy: 0.651600\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     0.6967 Validation Accuracy: 0.642800\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss:     0.6850 Validation Accuracy: 0.650000\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss:     0.6630 Validation Accuracy: 0.653000\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss:     0.6958 Validation Accuracy: 0.652000\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss:     0.6291 Validation Accuracy: 0.643600\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     0.6796 Validation Accuracy: 0.656600\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss:     0.6779 Validation Accuracy: 0.647400\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss:     0.6807 Validation Accuracy: 0.650600\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss:     0.6802 Validation Accuracy: 0.648800\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss:     0.6257 Validation Accuracy: 0.651400\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     0.6718 Validation Accuracy: 0.650600\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss:     0.6585 Validation Accuracy: 0.649800\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss:     0.6768 Validation Accuracy: 0.642400\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss:     0.6816 Validation Accuracy: 0.651600\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss:     0.6123 Validation Accuracy: 0.651600\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     0.6658 Validation Accuracy: 0.655800\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss:     0.6489 Validation Accuracy: 0.653200\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss:     0.6402 Validation Accuracy: 0.647200\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss:     0.6652 Validation Accuracy: 0.657800\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss:     0.6019 Validation Accuracy: 0.657000\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     0.6674 Validation Accuracy: 0.658000\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss:     0.6574 Validation Accuracy: 0.641400\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss:     0.6387 Validation Accuracy: 0.649600\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss:     0.6623 Validation Accuracy: 0.653600\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss:     0.6028 Validation Accuracy: 0.654000\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     0.6615 Validation Accuracy: 0.654200\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss:     0.6437 Validation Accuracy: 0.641400\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss:     0.6499 Validation Accuracy: 0.649400\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss:     0.6538 Validation Accuracy: 0.653800\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss:     0.5919 Validation Accuracy: 0.650600\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     0.6717 Validation Accuracy: 0.658000\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss:     0.6413 Validation Accuracy: 0.640800\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss:     0.6840 Validation Accuracy: 0.638800\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss:     0.6623 Validation Accuracy: 0.659800\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss:     0.6078 Validation Accuracy: 0.643200\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     0.6448 Validation Accuracy: 0.663200\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss:     0.6328 Validation Accuracy: 0.645400\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss:     0.6672 Validation Accuracy: 0.646200\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss:     0.6649 Validation Accuracy: 0.656800\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss:     0.6151 Validation Accuracy: 0.638600\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     0.6514 Validation Accuracy: 0.657600\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss:     0.6448 Validation Accuracy: 0.644400\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss:     0.6602 Validation Accuracy: 0.647200\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss:     0.6841 Validation Accuracy: 0.651200\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss:     0.6043 Validation Accuracy: 0.636000\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     0.6586 Validation Accuracy: 0.655200\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss:     0.6512 Validation Accuracy: 0.647600\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss:     0.6507 Validation Accuracy: 0.653200\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss:     0.6719 Validation Accuracy: 0.658200\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss:     0.6115 Validation Accuracy: 0.643200\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     0.6744 Validation Accuracy: 0.642600\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss:     0.6258 Validation Accuracy: 0.650800\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss:     0.6230 Validation Accuracy: 0.648200\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss:     0.6411 Validation Accuracy: 0.659400\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss:     0.5994 Validation Accuracy: 0.647400\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     0.6498 Validation Accuracy: 0.653200\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss:     0.6425 Validation Accuracy: 0.647000\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss:     0.6425 Validation Accuracy: 0.648400\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss:     0.6523 Validation Accuracy: 0.656200\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss:     0.5648 Validation Accuracy: 0.654600\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     0.6151 Validation Accuracy: 0.656200\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss:     0.6267 Validation Accuracy: 0.643000\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss:     0.6063 Validation Accuracy: 0.656400\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss:     0.6596 Validation Accuracy: 0.655400\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss:     0.5749 Validation Accuracy: 0.643000\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     0.6213 Validation Accuracy: 0.659600\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss:     0.6232 Validation Accuracy: 0.656200\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss:     0.5939 Validation Accuracy: 0.655600\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss:     0.6210 Validation Accuracy: 0.655200\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss:     0.5796 Validation Accuracy: 0.658200\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     0.6263 Validation Accuracy: 0.654800\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss:     0.5966 Validation Accuracy: 0.655200\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss:     0.5883 Validation Accuracy: 0.651000\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss:     0.6222 Validation Accuracy: 0.663800\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss:     0.5647 Validation Accuracy: 0.662200\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     0.6158 Validation Accuracy: 0.659800\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss:     0.6034 Validation Accuracy: 0.650400\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss:     0.5936 Validation Accuracy: 0.652000\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss:     0.6232 Validation Accuracy: 0.654600\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss:     0.5527 Validation Accuracy: 0.656800\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     0.6070 Validation Accuracy: 0.662600\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss:     0.5913 Validation Accuracy: 0.653600\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss:     0.5865 Validation Accuracy: 0.654600\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss:     0.6116 Validation Accuracy: 0.657200\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss:     0.5446 Validation Accuracy: 0.655400\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     0.6032 Validation Accuracy: 0.662400\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss:     0.5825 Validation Accuracy: 0.653400\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss:     0.5738 Validation Accuracy: 0.658800\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss:     0.6224 Validation Accuracy: 0.656800\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss:     0.5536 Validation Accuracy: 0.656200\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     0.5986 Validation Accuracy: 0.661400\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss:     0.5931 Validation Accuracy: 0.653200\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss:     0.5844 Validation Accuracy: 0.648800\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss:     0.6105 Validation Accuracy: 0.650200\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss:     0.5666 Validation Accuracy: 0.652200\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     0.5968 Validation Accuracy: 0.659400\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss:     0.5850 Validation Accuracy: 0.653600\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss:     0.5631 Validation Accuracy: 0.656200\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss:     0.6092 Validation Accuracy: 0.654800\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss:     0.5470 Validation Accuracy: 0.662200\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     0.5909 Validation Accuracy: 0.662600\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss:     0.6036 Validation Accuracy: 0.643400\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss:     0.5854 Validation Accuracy: 0.659200\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss:     0.6073 Validation Accuracy: 0.652400\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss:     0.5715 Validation Accuracy: 0.649200\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     0.5944 Validation Accuracy: 0.656400\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss:     0.5824 Validation Accuracy: 0.652800\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss:     0.5937 Validation Accuracy: 0.653600\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss:     0.6083 Validation Accuracy: 0.649600\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss:     0.5829 Validation Accuracy: 0.645000\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     0.5971 Validation Accuracy: 0.661000\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss:     0.5968 Validation Accuracy: 0.649800\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss:     0.5756 Validation Accuracy: 0.652400\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss:     0.6122 Validation Accuracy: 0.649800\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss:     0.5627 Validation Accuracy: 0.647200\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     0.5831 Validation Accuracy: 0.657800\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss:     0.5870 Validation Accuracy: 0.653400\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss:     0.5820 Validation Accuracy: 0.650400\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss:     0.6241 Validation Accuracy: 0.640000\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss:     0.5779 Validation Accuracy: 0.645000\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     0.5755 Validation Accuracy: 0.661800\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss:     0.5909 Validation Accuracy: 0.650800\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss:     0.5729 Validation Accuracy: 0.642200\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss:     0.6048 Validation Accuracy: 0.649000\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss:     0.5526 Validation Accuracy: 0.647600\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     0.5774 Validation Accuracy: 0.660800\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss:     0.5810 Validation Accuracy: 0.649000\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss:     0.5741 Validation Accuracy: 0.647800\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss:     0.5991 Validation Accuracy: 0.645800\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss:     0.5633 Validation Accuracy: 0.650800\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     0.5711 Validation Accuracy: 0.656600\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss:     0.5717 Validation Accuracy: 0.651800\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss:     0.5540 Validation Accuracy: 0.654200\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss:     0.5692 Validation Accuracy: 0.652200\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss:     0.5571 Validation Accuracy: 0.650400\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     0.5564 Validation Accuracy: 0.660800\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss:     0.5379 Validation Accuracy: 0.657400\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss:     0.5428 Validation Accuracy: 0.650000\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss:     0.5694 Validation Accuracy: 0.651400\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss:     0.5206 Validation Accuracy: 0.654600\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     0.5416 Validation Accuracy: 0.659200\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss:     0.5572 Validation Accuracy: 0.650400\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss:     0.5437 Validation Accuracy: 0.653600\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss:     0.5685 Validation Accuracy: 0.650200\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss:     0.5445 Validation Accuracy: 0.650800\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     0.5613 Validation Accuracy: 0.652000\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss:     0.5517 Validation Accuracy: 0.651400\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss:     0.5375 Validation Accuracy: 0.651400\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss:     0.5671 Validation Accuracy: 0.646400\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss:     0.5295 Validation Accuracy: 0.648000\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     0.5429 Validation Accuracy: 0.660400\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss:     0.5575 Validation Accuracy: 0.649000\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss:     0.5201 Validation Accuracy: 0.658800\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss:     0.5419 Validation Accuracy: 0.654200\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss:     0.5103 Validation Accuracy: 0.648800\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     0.5330 Validation Accuracy: 0.665400\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss:     0.5491 Validation Accuracy: 0.655000\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss:     0.5186 Validation Accuracy: 0.658600\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss:     0.5813 Validation Accuracy: 0.638800\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss:     0.5072 Validation Accuracy: 0.651800\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     0.5293 Validation Accuracy: 0.659800\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss:     0.5225 Validation Accuracy: 0.657200\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss:     0.5035 Validation Accuracy: 0.658000\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss:     0.5520 Validation Accuracy: 0.644400\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss:     0.5067 Validation Accuracy: 0.649200\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     0.5385 Validation Accuracy: 0.660000\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss:     0.5170 Validation Accuracy: 0.658000\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss:     0.4891 Validation Accuracy: 0.663400\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss:     0.5333 Validation Accuracy: 0.654200\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss:     0.4805 Validation Accuracy: 0.656200\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     0.5089 Validation Accuracy: 0.665000\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss:     0.5114 Validation Accuracy: 0.659400\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss:     0.4844 Validation Accuracy: 0.660800\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss:     0.5296 Validation Accuracy: 0.655000\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss:     0.4745 Validation Accuracy: 0.657000\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     0.5042 Validation Accuracy: 0.663400\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss:     0.5108 Validation Accuracy: 0.657400\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss:     0.5206 Validation Accuracy: 0.649200\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss:     0.5150 Validation Accuracy: 0.651200\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss:     0.4822 Validation Accuracy: 0.659800\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     0.4925 Validation Accuracy: 0.663200\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss:     0.5074 Validation Accuracy: 0.657200\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss:     0.4974 Validation Accuracy: 0.658800\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss:     0.5148 Validation Accuracy: 0.655600\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss:     0.4611 Validation Accuracy: 0.656600\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     0.4964 Validation Accuracy: 0.666400\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss:     0.5006 Validation Accuracy: 0.661000\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss:     0.4862 Validation Accuracy: 0.666200\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss:     0.5041 Validation Accuracy: 0.654600\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss:     0.4761 Validation Accuracy: 0.657000\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     0.4936 Validation Accuracy: 0.664600\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss:     0.5041 Validation Accuracy: 0.656400\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss:     0.4864 Validation Accuracy: 0.658200\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss:     0.5044 Validation Accuracy: 0.655400\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss:     0.4570 Validation Accuracy: 0.658000\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     0.4696 Validation Accuracy: 0.664800\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss:     0.5009 Validation Accuracy: 0.652600\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss:     0.4781 Validation Accuracy: 0.658000\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss:     0.5058 Validation Accuracy: 0.652800\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss:     0.4495 Validation Accuracy: 0.656800\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     0.4835 Validation Accuracy: 0.664000\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss:     0.5026 Validation Accuracy: 0.658200\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss:     0.4702 Validation Accuracy: 0.663200\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss:     0.5085 Validation Accuracy: 0.646400\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss:     0.4561 Validation Accuracy: 0.656600\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     0.4820 Validation Accuracy: 0.658800\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss:     0.4820 Validation Accuracy: 0.659400\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss:     0.4588 Validation Accuracy: 0.665000\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss:     0.4891 Validation Accuracy: 0.652000\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss:     0.4525 Validation Accuracy: 0.661000\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     0.4725 Validation Accuracy: 0.667400\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss:     0.4887 Validation Accuracy: 0.657600\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss:     0.4489 Validation Accuracy: 0.660800\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss:     0.4877 Validation Accuracy: 0.659600\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss:     0.4383 Validation Accuracy: 0.662200\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     0.4655 Validation Accuracy: 0.669000\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss:     0.4704 Validation Accuracy: 0.659200\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss:     0.4594 Validation Accuracy: 0.662400\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss:     0.4803 Validation Accuracy: 0.652800\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss:     0.4727 Validation Accuracy: 0.649200\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     0.4721 Validation Accuracy: 0.658400\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss:     0.4685 Validation Accuracy: 0.660200\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss:     0.4401 Validation Accuracy: 0.668400\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss:     0.4851 Validation Accuracy: 0.655000\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss:     0.4618 Validation Accuracy: 0.651600\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     0.4810 Validation Accuracy: 0.660000\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss:     0.4869 Validation Accuracy: 0.651600\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss:     0.4691 Validation Accuracy: 0.660200\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss:     0.4768 Validation Accuracy: 0.661200\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss:     0.4560 Validation Accuracy: 0.652600\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     0.4659 Validation Accuracy: 0.661000\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss:     0.4636 Validation Accuracy: 0.658400\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss:     0.4490 Validation Accuracy: 0.663200\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss:     0.5101 Validation Accuracy: 0.646400\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss:     0.4559 Validation Accuracy: 0.649800\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     0.4831 Validation Accuracy: 0.660000\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss:     0.4735 Validation Accuracy: 0.652200\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss:     0.4602 Validation Accuracy: 0.656200\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss:     0.4831 Validation Accuracy: 0.652400\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss:     0.4404 Validation Accuracy: 0.655000\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     0.4830 Validation Accuracy: 0.657600\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss:     0.4761 Validation Accuracy: 0.659800\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss:     0.4487 Validation Accuracy: 0.660000\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss:     0.4743 Validation Accuracy: 0.650000\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss:     0.4409 Validation Accuracy: 0.654200\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     0.4766 Validation Accuracy: 0.655600\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss:     0.4498 Validation Accuracy: 0.660600\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss:     0.4424 Validation Accuracy: 0.659000\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss:     0.4618 Validation Accuracy: 0.660000\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss:     0.4504 Validation Accuracy: 0.651400\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     0.4551 Validation Accuracy: 0.669400\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss:     0.4486 Validation Accuracy: 0.655000\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss:     0.4329 Validation Accuracy: 0.655800\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss:     0.4514 Validation Accuracy: 0.659400\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss:     0.4209 Validation Accuracy: 0.652600\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     0.4583 Validation Accuracy: 0.662800\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss:     0.4329 Validation Accuracy: 0.657200\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss:     0.4336 Validation Accuracy: 0.662600\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss:     0.4702 Validation Accuracy: 0.653400\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss:     0.4205 Validation Accuracy: 0.657800\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     0.4696 Validation Accuracy: 0.660000\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss:     0.4391 Validation Accuracy: 0.662800\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss:     0.4360 Validation Accuracy: 0.665800\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss:     0.4655 Validation Accuracy: 0.649600\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss:     0.4159 Validation Accuracy: 0.660000\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss:     0.4630 Validation Accuracy: 0.666400\n",
      "Epoch 101, CIFAR-10 Batch 2:  Loss:     0.4479 Validation Accuracy: 0.658000\n",
      "Epoch 101, CIFAR-10 Batch 3:  Loss:     0.4190 Validation Accuracy: 0.673200\n",
      "Epoch 101, CIFAR-10 Batch 4:  Loss:     0.4518 Validation Accuracy: 0.655400\n",
      "Epoch 101, CIFAR-10 Batch 5:  Loss:     0.4188 Validation Accuracy: 0.656400\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss:     0.4580 Validation Accuracy: 0.663800\n",
      "Epoch 102, CIFAR-10 Batch 2:  Loss:     0.4293 Validation Accuracy: 0.661600\n",
      "Epoch 102, CIFAR-10 Batch 3:  Loss:     0.4341 Validation Accuracy: 0.661400\n",
      "Epoch 102, CIFAR-10 Batch 4:  Loss:     0.4437 Validation Accuracy: 0.657600\n",
      "Epoch 102, CIFAR-10 Batch 5:  Loss:     0.3929 Validation Accuracy: 0.667200\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss:     0.4320 Validation Accuracy: 0.668200\n",
      "Epoch 103, CIFAR-10 Batch 2:  Loss:     0.4141 Validation Accuracy: 0.665600\n",
      "Epoch 103, CIFAR-10 Batch 3:  Loss:     0.4040 Validation Accuracy: 0.673800\n",
      "Epoch 103, CIFAR-10 Batch 4:  Loss:     0.4408 Validation Accuracy: 0.650600\n",
      "Epoch 103, CIFAR-10 Batch 5:  Loss:     0.4342 Validation Accuracy: 0.654000\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss:     0.4566 Validation Accuracy: 0.669600\n",
      "Epoch 104, CIFAR-10 Batch 2:  Loss:     0.4219 Validation Accuracy: 0.660000\n",
      "Epoch 104, CIFAR-10 Batch 3:  Loss:     0.4234 Validation Accuracy: 0.658400\n",
      "Epoch 104, CIFAR-10 Batch 4:  Loss:     0.4713 Validation Accuracy: 0.647800\n",
      "Epoch 104, CIFAR-10 Batch 5:  Loss:     0.4171 Validation Accuracy: 0.653000\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss:     0.4610 Validation Accuracy: 0.660400\n",
      "Epoch 105, CIFAR-10 Batch 2:  Loss:     0.4289 Validation Accuracy: 0.659000\n",
      "Epoch 105, CIFAR-10 Batch 3:  Loss:     0.4223 Validation Accuracy: 0.660200\n",
      "Epoch 105, CIFAR-10 Batch 4:  Loss:     0.4570 Validation Accuracy: 0.645400\n",
      "Epoch 105, CIFAR-10 Batch 5:  Loss:     0.4606 Validation Accuracy: 0.646000\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss:     0.4776 Validation Accuracy: 0.657600\n",
      "Epoch 106, CIFAR-10 Batch 2:  Loss:     0.4164 Validation Accuracy: 0.662200\n",
      "Epoch 106, CIFAR-10 Batch 3:  Loss:     0.4132 Validation Accuracy: 0.660400\n",
      "Epoch 106, CIFAR-10 Batch 4:  Loss:     0.4657 Validation Accuracy: 0.648400\n",
      "Epoch 106, CIFAR-10 Batch 5:  Loss:     0.4486 Validation Accuracy: 0.651400\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss:     0.4668 Validation Accuracy: 0.662800\n",
      "Epoch 107, CIFAR-10 Batch 2:  Loss:     0.4054 Validation Accuracy: 0.664600\n",
      "Epoch 107, CIFAR-10 Batch 3:  Loss:     0.4191 Validation Accuracy: 0.656200\n",
      "Epoch 107, CIFAR-10 Batch 4:  Loss:     0.4678 Validation Accuracy: 0.644400\n",
      "Epoch 107, CIFAR-10 Batch 5:  Loss:     0.4346 Validation Accuracy: 0.654800\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss:     0.4753 Validation Accuracy: 0.654200\n",
      "Epoch 108, CIFAR-10 Batch 2:  Loss:     0.4259 Validation Accuracy: 0.665000\n",
      "Epoch 108, CIFAR-10 Batch 3:  Loss:     0.4197 Validation Accuracy: 0.665600\n",
      "Epoch 108, CIFAR-10 Batch 4:  Loss:     0.4919 Validation Accuracy: 0.635000\n",
      "Epoch 108, CIFAR-10 Batch 5:  Loss:     0.4514 Validation Accuracy: 0.638600\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss:     0.5095 Validation Accuracy: 0.655000\n",
      "Epoch 109, CIFAR-10 Batch 2:  Loss:     0.4513 Validation Accuracy: 0.656600\n",
      "Epoch 109, CIFAR-10 Batch 3:  Loss:     0.4223 Validation Accuracy: 0.663600\n",
      "Epoch 109, CIFAR-10 Batch 4:  Loss:     0.4581 Validation Accuracy: 0.645400\n",
      "Epoch 109, CIFAR-10 Batch 5:  Loss:     0.4600 Validation Accuracy: 0.645000\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss:     0.5055 Validation Accuracy: 0.649600\n",
      "Epoch 110, CIFAR-10 Batch 2:  Loss:     0.4236 Validation Accuracy: 0.656600\n",
      "Epoch 110, CIFAR-10 Batch 3:  Loss:     0.4244 Validation Accuracy: 0.664400\n",
      "Epoch 110, CIFAR-10 Batch 4:  Loss:     0.4636 Validation Accuracy: 0.649800\n",
      "Epoch 110, CIFAR-10 Batch 5:  Loss:     0.4163 Validation Accuracy: 0.649800\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss:     0.5016 Validation Accuracy: 0.648600\n",
      "Epoch 111, CIFAR-10 Batch 2:  Loss:     0.4592 Validation Accuracy: 0.656200\n",
      "Epoch 111, CIFAR-10 Batch 3:  Loss:     0.4108 Validation Accuracy: 0.664000\n",
      "Epoch 111, CIFAR-10 Batch 4:  Loss:     0.4359 Validation Accuracy: 0.656400\n",
      "Epoch 111, CIFAR-10 Batch 5:  Loss:     0.4317 Validation Accuracy: 0.648200\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss:     0.4351 Validation Accuracy: 0.661200\n",
      "Epoch 112, CIFAR-10 Batch 2:  Loss:     0.4337 Validation Accuracy: 0.650600\n",
      "Epoch 112, CIFAR-10 Batch 3:  Loss:     0.3980 Validation Accuracy: 0.663800\n",
      "Epoch 112, CIFAR-10 Batch 4:  Loss:     0.4355 Validation Accuracy: 0.657400\n",
      "Epoch 112, CIFAR-10 Batch 5:  Loss:     0.3903 Validation Accuracy: 0.650200\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss:     0.4417 Validation Accuracy: 0.664400\n",
      "Epoch 113, CIFAR-10 Batch 2:  Loss:     0.4457 Validation Accuracy: 0.653000\n",
      "Epoch 113, CIFAR-10 Batch 3:  Loss:     0.4422 Validation Accuracy: 0.655800\n",
      "Epoch 113, CIFAR-10 Batch 4:  Loss:     0.4189 Validation Accuracy: 0.661200\n",
      "Epoch 113, CIFAR-10 Batch 5:  Loss:     0.4157 Validation Accuracy: 0.647000\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss:     0.4434 Validation Accuracy: 0.651800\n",
      "Epoch 114, CIFAR-10 Batch 2:  Loss:     0.4067 Validation Accuracy: 0.665400\n",
      "Epoch 114, CIFAR-10 Batch 3:  Loss:     0.4112 Validation Accuracy: 0.657400\n",
      "Epoch 114, CIFAR-10 Batch 4:  Loss:     0.4139 Validation Accuracy: 0.669000\n",
      "Epoch 114, CIFAR-10 Batch 5:  Loss:     0.3964 Validation Accuracy: 0.649400\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss:     0.4308 Validation Accuracy: 0.654400\n",
      "Epoch 115, CIFAR-10 Batch 2:  Loss:     0.4084 Validation Accuracy: 0.667200\n",
      "Epoch 115, CIFAR-10 Batch 3:  Loss:     0.4091 Validation Accuracy: 0.657800\n",
      "Epoch 115, CIFAR-10 Batch 4:  Loss:     0.4205 Validation Accuracy: 0.659800\n",
      "Epoch 115, CIFAR-10 Batch 5:  Loss:     0.4125 Validation Accuracy: 0.644800\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss:     0.4322 Validation Accuracy: 0.659200\n",
      "Epoch 116, CIFAR-10 Batch 2:  Loss:     0.3913 Validation Accuracy: 0.667400\n",
      "Epoch 116, CIFAR-10 Batch 3:  Loss:     0.3798 Validation Accuracy: 0.663000\n",
      "Epoch 116, CIFAR-10 Batch 4:  Loss:     0.4043 Validation Accuracy: 0.658200\n",
      "Epoch 116, CIFAR-10 Batch 5:  Loss:     0.3975 Validation Accuracy: 0.641400\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss:     0.4323 Validation Accuracy: 0.658600\n",
      "Epoch 117, CIFAR-10 Batch 2:  Loss:     0.3981 Validation Accuracy: 0.661600\n",
      "Epoch 117, CIFAR-10 Batch 3:  Loss:     0.3981 Validation Accuracy: 0.654400\n",
      "Epoch 117, CIFAR-10 Batch 4:  Loss:     0.4006 Validation Accuracy: 0.666400\n",
      "Epoch 117, CIFAR-10 Batch 5:  Loss:     0.3736 Validation Accuracy: 0.651600\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss:     0.4274 Validation Accuracy: 0.657200\n",
      "Epoch 118, CIFAR-10 Batch 2:  Loss:     0.3996 Validation Accuracy: 0.662200\n",
      "Epoch 118, CIFAR-10 Batch 3:  Loss:     0.3900 Validation Accuracy: 0.657600\n",
      "Epoch 118, CIFAR-10 Batch 4:  Loss:     0.4055 Validation Accuracy: 0.662800\n",
      "Epoch 118, CIFAR-10 Batch 5:  Loss:     0.4063 Validation Accuracy: 0.642800\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss:     0.4333 Validation Accuracy: 0.648000\n",
      "Epoch 119, CIFAR-10 Batch 2:  Loss:     0.3970 Validation Accuracy: 0.658000\n",
      "Epoch 119, CIFAR-10 Batch 3:  Loss:     0.3859 Validation Accuracy: 0.657800\n",
      "Epoch 119, CIFAR-10 Batch 4:  Loss:     0.4170 Validation Accuracy: 0.658000\n",
      "Epoch 119, CIFAR-10 Batch 5:  Loss:     0.3788 Validation Accuracy: 0.654400\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss:     0.4183 Validation Accuracy: 0.658800\n",
      "Epoch 120, CIFAR-10 Batch 2:  Loss:     0.3860 Validation Accuracy: 0.660400\n",
      "Epoch 120, CIFAR-10 Batch 3:  Loss:     0.3775 Validation Accuracy: 0.660200\n",
      "Epoch 120, CIFAR-10 Batch 4:  Loss:     0.4082 Validation Accuracy: 0.662800\n",
      "Epoch 120, CIFAR-10 Batch 5:  Loss:     0.3702 Validation Accuracy: 0.656600\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss:     0.4220 Validation Accuracy: 0.648600\n",
      "Epoch 121, CIFAR-10 Batch 2:  Loss:     0.3986 Validation Accuracy: 0.659200\n",
      "Epoch 121, CIFAR-10 Batch 3:  Loss:     0.3750 Validation Accuracy: 0.653000\n",
      "Epoch 121, CIFAR-10 Batch 4:  Loss:     0.4572 Validation Accuracy: 0.646400\n",
      "Epoch 121, CIFAR-10 Batch 5:  Loss:     0.3971 Validation Accuracy: 0.648200\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss:     0.4297 Validation Accuracy: 0.649200\n",
      "Epoch 122, CIFAR-10 Batch 2:  Loss:     0.3999 Validation Accuracy: 0.653200\n",
      "Epoch 122, CIFAR-10 Batch 3:  Loss:     0.3931 Validation Accuracy: 0.649400\n",
      "Epoch 122, CIFAR-10 Batch 4:  Loss:     0.4605 Validation Accuracy: 0.643800\n",
      "Epoch 122, CIFAR-10 Batch 5:  Loss:     0.3855 Validation Accuracy: 0.648800\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss:     0.4007 Validation Accuracy: 0.656400\n",
      "Epoch 123, CIFAR-10 Batch 2:  Loss:     0.3808 Validation Accuracy: 0.666600\n",
      "Epoch 123, CIFAR-10 Batch 3:  Loss:     0.3760 Validation Accuracy: 0.657200\n",
      "Epoch 123, CIFAR-10 Batch 4:  Loss:     0.4380 Validation Accuracy: 0.661600\n",
      "Epoch 123, CIFAR-10 Batch 5:  Loss:     0.3730 Validation Accuracy: 0.652800\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss:     0.4109 Validation Accuracy: 0.665400\n",
      "Epoch 124, CIFAR-10 Batch 2:  Loss:     0.3746 Validation Accuracy: 0.668200\n",
      "Epoch 124, CIFAR-10 Batch 3:  Loss:     0.3757 Validation Accuracy: 0.654600\n",
      "Epoch 124, CIFAR-10 Batch 4:  Loss:     0.4109 Validation Accuracy: 0.667000\n",
      "Epoch 124, CIFAR-10 Batch 5:  Loss:     0.3659 Validation Accuracy: 0.654600\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss:     0.3945 Validation Accuracy: 0.654800\n",
      "Epoch 125, CIFAR-10 Batch 2:  Loss:     0.3772 Validation Accuracy: 0.665200\n",
      "Epoch 125, CIFAR-10 Batch 3:  Loss:     0.3622 Validation Accuracy: 0.662200\n",
      "Epoch 125, CIFAR-10 Batch 4:  Loss:     0.4021 Validation Accuracy: 0.665600\n",
      "Epoch 125, CIFAR-10 Batch 5:  Loss:     0.3408 Validation Accuracy: 0.660200\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss:     0.3816 Validation Accuracy: 0.662400\n",
      "Epoch 126, CIFAR-10 Batch 2:  Loss:     0.3799 Validation Accuracy: 0.667800\n",
      "Epoch 126, CIFAR-10 Batch 3:  Loss:     0.3670 Validation Accuracy: 0.662000\n",
      "Epoch 126, CIFAR-10 Batch 4:  Loss:     0.4004 Validation Accuracy: 0.662400\n",
      "Epoch 126, CIFAR-10 Batch 5:  Loss:     0.3480 Validation Accuracy: 0.659600\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss:     0.3890 Validation Accuracy: 0.659600\n",
      "Epoch 127, CIFAR-10 Batch 2:  Loss:     0.3859 Validation Accuracy: 0.668600\n",
      "Epoch 127, CIFAR-10 Batch 3:  Loss:     0.3800 Validation Accuracy: 0.659800\n",
      "Epoch 127, CIFAR-10 Batch 4:  Loss:     0.3833 Validation Accuracy: 0.674400\n",
      "Epoch 127, CIFAR-10 Batch 5:  Loss:     0.3337 Validation Accuracy: 0.661800\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss:     0.3715 Validation Accuracy: 0.661200\n",
      "Epoch 128, CIFAR-10 Batch 2:  Loss:     0.3693 Validation Accuracy: 0.661400\n",
      "Epoch 128, CIFAR-10 Batch 3:  Loss:     0.3590 Validation Accuracy: 0.664400\n",
      "Epoch 128, CIFAR-10 Batch 4:  Loss:     0.3779 Validation Accuracy: 0.672600\n",
      "Epoch 128, CIFAR-10 Batch 5:  Loss:     0.3438 Validation Accuracy: 0.659600\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss:     0.3632 Validation Accuracy: 0.661800\n",
      "Epoch 129, CIFAR-10 Batch 2:  Loss:     0.3601 Validation Accuracy: 0.666400\n",
      "Epoch 129, CIFAR-10 Batch 3:  Loss:     0.3427 Validation Accuracy: 0.663600\n",
      "Epoch 129, CIFAR-10 Batch 4:  Loss:     0.3852 Validation Accuracy: 0.657600\n",
      "Epoch 129, CIFAR-10 Batch 5:  Loss:     0.3342 Validation Accuracy: 0.658400\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss:     0.3456 Validation Accuracy: 0.664600\n",
      "Epoch 130, CIFAR-10 Batch 2:  Loss:     0.3447 Validation Accuracy: 0.666400\n",
      "Epoch 130, CIFAR-10 Batch 3:  Loss:     0.3304 Validation Accuracy: 0.668400\n",
      "Epoch 130, CIFAR-10 Batch 4:  Loss:     0.3898 Validation Accuracy: 0.654000\n",
      "Epoch 130, CIFAR-10 Batch 5:  Loss:     0.3531 Validation Accuracy: 0.655400\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss:     0.3776 Validation Accuracy: 0.659800\n",
      "Epoch 131, CIFAR-10 Batch 2:  Loss:     0.3456 Validation Accuracy: 0.665200\n",
      "Epoch 131, CIFAR-10 Batch 3:  Loss:     0.3279 Validation Accuracy: 0.667000\n",
      "Epoch 131, CIFAR-10 Batch 4:  Loss:     0.3774 Validation Accuracy: 0.664200\n",
      "Epoch 131, CIFAR-10 Batch 5:  Loss:     0.3467 Validation Accuracy: 0.654400\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss:     0.3703 Validation Accuracy: 0.658800\n",
      "Epoch 132, CIFAR-10 Batch 2:  Loss:     0.3609 Validation Accuracy: 0.658200\n",
      "Epoch 132, CIFAR-10 Batch 3:  Loss:     0.3384 Validation Accuracy: 0.655200\n",
      "Epoch 132, CIFAR-10 Batch 4:  Loss:     0.3700 Validation Accuracy: 0.665800\n",
      "Epoch 132, CIFAR-10 Batch 5:  Loss:     0.3446 Validation Accuracy: 0.657400\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss:     0.3650 Validation Accuracy: 0.663200\n",
      "Epoch 133, CIFAR-10 Batch 2:  Loss:     0.3490 Validation Accuracy: 0.664200\n",
      "Epoch 133, CIFAR-10 Batch 3:  Loss:     0.3495 Validation Accuracy: 0.657400\n",
      "Epoch 133, CIFAR-10 Batch 4:  Loss:     0.3751 Validation Accuracy: 0.665600\n",
      "Epoch 133, CIFAR-10 Batch 5:  Loss:     0.3563 Validation Accuracy: 0.649600\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss:     0.3640 Validation Accuracy: 0.659000\n",
      "Epoch 134, CIFAR-10 Batch 2:  Loss:     0.3658 Validation Accuracy: 0.660600\n",
      "Epoch 134, CIFAR-10 Batch 3:  Loss:     0.3415 Validation Accuracy: 0.649000\n",
      "Epoch 134, CIFAR-10 Batch 4:  Loss:     0.3606 Validation Accuracy: 0.660600\n",
      "Epoch 134, CIFAR-10 Batch 5:  Loss:     0.3197 Validation Accuracy: 0.659000\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss:     0.3589 Validation Accuracy: 0.660800\n",
      "Epoch 135, CIFAR-10 Batch 2:  Loss:     0.3814 Validation Accuracy: 0.657600\n",
      "Epoch 135, CIFAR-10 Batch 3:  Loss:     0.3673 Validation Accuracy: 0.648800\n",
      "Epoch 135, CIFAR-10 Batch 4:  Loss:     0.3878 Validation Accuracy: 0.646600\n",
      "Epoch 135, CIFAR-10 Batch 5:  Loss:     0.3329 Validation Accuracy: 0.657200\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss:     0.3525 Validation Accuracy: 0.652200\n",
      "Epoch 136, CIFAR-10 Batch 2:  Loss:     0.3465 Validation Accuracy: 0.663200\n",
      "Epoch 136, CIFAR-10 Batch 3:  Loss:     0.3498 Validation Accuracy: 0.651000\n",
      "Epoch 136, CIFAR-10 Batch 4:  Loss:     0.3743 Validation Accuracy: 0.662200\n",
      "Epoch 136, CIFAR-10 Batch 5:  Loss:     0.3159 Validation Accuracy: 0.665800\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss:     0.3432 Validation Accuracy: 0.654200\n",
      "Epoch 137, CIFAR-10 Batch 2:  Loss:     0.3284 Validation Accuracy: 0.665800\n",
      "Epoch 137, CIFAR-10 Batch 3:  Loss:     0.3239 Validation Accuracy: 0.661800\n",
      "Epoch 137, CIFAR-10 Batch 4:  Loss:     0.3434 Validation Accuracy: 0.658400\n",
      "Epoch 137, CIFAR-10 Batch 5:  Loss:     0.3092 Validation Accuracy: 0.661200\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss:     0.3330 Validation Accuracy: 0.660800\n",
      "Epoch 138, CIFAR-10 Batch 2:  Loss:     0.3446 Validation Accuracy: 0.662800\n",
      "Epoch 138, CIFAR-10 Batch 3:  Loss:     0.3288 Validation Accuracy: 0.653400\n",
      "Epoch 138, CIFAR-10 Batch 4:  Loss:     0.3546 Validation Accuracy: 0.660400\n",
      "Epoch 138, CIFAR-10 Batch 5:  Loss:     0.3284 Validation Accuracy: 0.658800\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss:     0.3636 Validation Accuracy: 0.650000\n",
      "Epoch 139, CIFAR-10 Batch 2:  Loss:     0.3313 Validation Accuracy: 0.664400\n",
      "Epoch 139, CIFAR-10 Batch 3:  Loss:     0.3124 Validation Accuracy: 0.654400\n",
      "Epoch 139, CIFAR-10 Batch 4:  Loss:     0.3536 Validation Accuracy: 0.653800\n",
      "Epoch 139, CIFAR-10 Batch 5:  Loss:     0.3267 Validation Accuracy: 0.660400\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss:     0.3702 Validation Accuracy: 0.650000\n",
      "Epoch 140, CIFAR-10 Batch 2:  Loss:     0.3380 Validation Accuracy: 0.665200\n",
      "Epoch 140, CIFAR-10 Batch 3:  Loss:     0.3160 Validation Accuracy: 0.653800\n",
      "Epoch 140, CIFAR-10 Batch 4:  Loss:     0.3511 Validation Accuracy: 0.650800\n",
      "Epoch 140, CIFAR-10 Batch 5:  Loss:     0.3157 Validation Accuracy: 0.655800\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss:     0.3429 Validation Accuracy: 0.657600\n",
      "Epoch 141, CIFAR-10 Batch 2:  Loss:     0.3269 Validation Accuracy: 0.656800\n",
      "Epoch 141, CIFAR-10 Batch 3:  Loss:     0.3077 Validation Accuracy: 0.659600\n",
      "Epoch 141, CIFAR-10 Batch 4:  Loss:     0.3514 Validation Accuracy: 0.655600\n",
      "Epoch 141, CIFAR-10 Batch 5:  Loss:     0.3136 Validation Accuracy: 0.661200\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss:     0.3519 Validation Accuracy: 0.654800\n",
      "Epoch 142, CIFAR-10 Batch 2:  Loss:     0.3183 Validation Accuracy: 0.662000\n",
      "Epoch 142, CIFAR-10 Batch 3:  Loss:     0.3164 Validation Accuracy: 0.647400\n",
      "Epoch 142, CIFAR-10 Batch 4:  Loss:     0.3401 Validation Accuracy: 0.653400\n",
      "Epoch 142, CIFAR-10 Batch 5:  Loss:     0.2954 Validation Accuracy: 0.663200\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss:     0.3402 Validation Accuracy: 0.656800\n",
      "Epoch 143, CIFAR-10 Batch 2:  Loss:     0.3326 Validation Accuracy: 0.656600\n",
      "Epoch 143, CIFAR-10 Batch 3:  Loss:     0.3108 Validation Accuracy: 0.651400\n",
      "Epoch 143, CIFAR-10 Batch 4:  Loss:     0.3379 Validation Accuracy: 0.650600\n",
      "Epoch 143, CIFAR-10 Batch 5:  Loss:     0.3079 Validation Accuracy: 0.657000\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss:     0.3391 Validation Accuracy: 0.654200\n",
      "Epoch 144, CIFAR-10 Batch 2:  Loss:     0.3200 Validation Accuracy: 0.660600\n",
      "Epoch 144, CIFAR-10 Batch 3:  Loss:     0.3229 Validation Accuracy: 0.649600\n",
      "Epoch 144, CIFAR-10 Batch 4:  Loss:     0.3464 Validation Accuracy: 0.649800\n",
      "Epoch 144, CIFAR-10 Batch 5:  Loss:     0.3074 Validation Accuracy: 0.654000\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss:     0.3323 Validation Accuracy: 0.651600\n",
      "Epoch 145, CIFAR-10 Batch 2:  Loss:     0.3306 Validation Accuracy: 0.653800\n",
      "Epoch 145, CIFAR-10 Batch 3:  Loss:     0.3141 Validation Accuracy: 0.653000\n",
      "Epoch 145, CIFAR-10 Batch 4:  Loss:     0.3327 Validation Accuracy: 0.648000\n",
      "Epoch 145, CIFAR-10 Batch 5:  Loss:     0.2952 Validation Accuracy: 0.659800\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss:     0.3280 Validation Accuracy: 0.659000\n",
      "Epoch 146, CIFAR-10 Batch 2:  Loss:     0.3052 Validation Accuracy: 0.664400\n",
      "Epoch 146, CIFAR-10 Batch 3:  Loss:     0.2979 Validation Accuracy: 0.655000\n",
      "Epoch 146, CIFAR-10 Batch 4:  Loss:     0.3267 Validation Accuracy: 0.652200\n",
      "Epoch 146, CIFAR-10 Batch 5:  Loss:     0.2876 Validation Accuracy: 0.658800\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss:     0.3050 Validation Accuracy: 0.657800\n",
      "Epoch 147, CIFAR-10 Batch 2:  Loss:     0.3160 Validation Accuracy: 0.654400\n",
      "Epoch 147, CIFAR-10 Batch 3:  Loss:     0.2929 Validation Accuracy: 0.660000\n",
      "Epoch 147, CIFAR-10 Batch 4:  Loss:     0.3413 Validation Accuracy: 0.645000\n",
      "Epoch 147, CIFAR-10 Batch 5:  Loss:     0.2972 Validation Accuracy: 0.656000\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss:     0.3118 Validation Accuracy: 0.663800\n",
      "Epoch 148, CIFAR-10 Batch 2:  Loss:     0.3136 Validation Accuracy: 0.663400\n",
      "Epoch 148, CIFAR-10 Batch 3:  Loss:     0.2993 Validation Accuracy: 0.654200\n",
      "Epoch 148, CIFAR-10 Batch 4:  Loss:     0.3330 Validation Accuracy: 0.644400\n",
      "Epoch 148, CIFAR-10 Batch 5:  Loss:     0.2867 Validation Accuracy: 0.654800\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss:     0.3061 Validation Accuracy: 0.661000\n",
      "Epoch 149, CIFAR-10 Batch 2:  Loss:     0.2894 Validation Accuracy: 0.661000\n",
      "Epoch 149, CIFAR-10 Batch 3:  Loss:     0.2902 Validation Accuracy: 0.657200\n",
      "Epoch 149, CIFAR-10 Batch 4:  Loss:     0.3416 Validation Accuracy: 0.652400\n",
      "Epoch 149, CIFAR-10 Batch 5:  Loss:     0.2983 Validation Accuracy: 0.658000\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss:     0.3057 Validation Accuracy: 0.652400\n",
      "Epoch 150, CIFAR-10 Batch 2:  Loss:     0.2957 Validation Accuracy: 0.657600\n",
      "Epoch 150, CIFAR-10 Batch 3:  Loss:     0.2753 Validation Accuracy: 0.657600\n",
      "Epoch 150, CIFAR-10 Batch 4:  Loss:     0.3220 Validation Accuracy: 0.651400\n",
      "Epoch 150, CIFAR-10 Batch 5:  Loss:     0.2898 Validation Accuracy: 0.650200\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss:     0.3128 Validation Accuracy: 0.654600\n",
      "Epoch 151, CIFAR-10 Batch 2:  Loss:     0.2980 Validation Accuracy: 0.660000\n",
      "Epoch 151, CIFAR-10 Batch 3:  Loss:     0.2770 Validation Accuracy: 0.658600\n",
      "Epoch 151, CIFAR-10 Batch 4:  Loss:     0.3305 Validation Accuracy: 0.659000\n",
      "Epoch 151, CIFAR-10 Batch 5:  Loss:     0.2735 Validation Accuracy: 0.652000\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss:     0.3077 Validation Accuracy: 0.654800\n",
      "Epoch 152, CIFAR-10 Batch 2:  Loss:     0.3038 Validation Accuracy: 0.653600\n",
      "Epoch 152, CIFAR-10 Batch 3:  Loss:     0.2774 Validation Accuracy: 0.653800\n",
      "Epoch 152, CIFAR-10 Batch 4:  Loss:     0.3162 Validation Accuracy: 0.655600\n",
      "Epoch 152, CIFAR-10 Batch 5:  Loss:     0.2637 Validation Accuracy: 0.663000\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss:     0.2992 Validation Accuracy: 0.650600\n",
      "Epoch 153, CIFAR-10 Batch 2:  Loss:     0.2988 Validation Accuracy: 0.657200\n",
      "Epoch 153, CIFAR-10 Batch 3:  Loss:     0.2893 Validation Accuracy: 0.657800\n",
      "Epoch 153, CIFAR-10 Batch 4:  Loss:     0.3180 Validation Accuracy: 0.649200\n",
      "Epoch 153, CIFAR-10 Batch 5:  Loss:     0.2746 Validation Accuracy: 0.664200\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss:     0.3029 Validation Accuracy: 0.654400\n",
      "Epoch 154, CIFAR-10 Batch 2:  Loss:     0.2992 Validation Accuracy: 0.659400\n",
      "Epoch 154, CIFAR-10 Batch 3:  Loss:     0.2830 Validation Accuracy: 0.659800\n",
      "Epoch 154, CIFAR-10 Batch 4:  Loss:     0.3052 Validation Accuracy: 0.654600\n",
      "Epoch 154, CIFAR-10 Batch 5:  Loss:     0.2664 Validation Accuracy: 0.663800\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss:     0.3033 Validation Accuracy: 0.661200\n",
      "Epoch 155, CIFAR-10 Batch 2:  Loss:     0.2857 Validation Accuracy: 0.660400\n",
      "Epoch 155, CIFAR-10 Batch 3:  Loss:     0.2732 Validation Accuracy: 0.656600\n",
      "Epoch 155, CIFAR-10 Batch 4:  Loss:     0.3373 Validation Accuracy: 0.651400\n",
      "Epoch 155, CIFAR-10 Batch 5:  Loss:     0.2716 Validation Accuracy: 0.659400\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss:     0.2906 Validation Accuracy: 0.667800\n",
      "Epoch 156, CIFAR-10 Batch 2:  Loss:     0.2792 Validation Accuracy: 0.661800\n",
      "Epoch 156, CIFAR-10 Batch 3:  Loss:     0.2680 Validation Accuracy: 0.654600\n",
      "Epoch 156, CIFAR-10 Batch 4:  Loss:     0.3122 Validation Accuracy: 0.658600\n",
      "Epoch 156, CIFAR-10 Batch 5:  Loss:     0.2647 Validation Accuracy: 0.657600\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss:     0.3038 Validation Accuracy: 0.660600\n",
      "Epoch 157, CIFAR-10 Batch 2:  Loss:     0.2900 Validation Accuracy: 0.665200\n",
      "Epoch 157, CIFAR-10 Batch 3:  Loss:     0.2671 Validation Accuracy: 0.661600\n",
      "Epoch 157, CIFAR-10 Batch 4:  Loss:     0.3122 Validation Accuracy: 0.650600\n",
      "Epoch 157, CIFAR-10 Batch 5:  Loss:     0.2764 Validation Accuracy: 0.665400\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss:     0.2862 Validation Accuracy: 0.656600\n",
      "Epoch 158, CIFAR-10 Batch 2:  Loss:     0.2703 Validation Accuracy: 0.667200\n",
      "Epoch 158, CIFAR-10 Batch 3:  Loss:     0.2708 Validation Accuracy: 0.660800\n",
      "Epoch 158, CIFAR-10 Batch 4:  Loss:     0.3115 Validation Accuracy: 0.651600\n",
      "Epoch 158, CIFAR-10 Batch 5:  Loss:     0.2634 Validation Accuracy: 0.658200\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss:     0.2794 Validation Accuracy: 0.669000\n",
      "Epoch 159, CIFAR-10 Batch 2:  Loss:     0.2766 Validation Accuracy: 0.657200\n",
      "Epoch 159, CIFAR-10 Batch 3:  Loss:     0.2673 Validation Accuracy: 0.658000\n",
      "Epoch 159, CIFAR-10 Batch 4:  Loss:     0.3048 Validation Accuracy: 0.663600\n",
      "Epoch 159, CIFAR-10 Batch 5:  Loss:     0.2633 Validation Accuracy: 0.659400\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss:     0.2885 Validation Accuracy: 0.658200\n",
      "Epoch 160, CIFAR-10 Batch 2:  Loss:     0.3053 Validation Accuracy: 0.655000\n",
      "Epoch 160, CIFAR-10 Batch 3:  Loss:     0.2774 Validation Accuracy: 0.652400\n",
      "Epoch 160, CIFAR-10 Batch 4:  Loss:     0.2972 Validation Accuracy: 0.659600\n",
      "Epoch 160, CIFAR-10 Batch 5:  Loss:     0.2750 Validation Accuracy: 0.647800\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss:     0.2841 Validation Accuracy: 0.658200\n",
      "Epoch 161, CIFAR-10 Batch 2:  Loss:     0.2733 Validation Accuracy: 0.660400\n",
      "Epoch 161, CIFAR-10 Batch 3:  Loss:     0.2659 Validation Accuracy: 0.657200\n",
      "Epoch 161, CIFAR-10 Batch 4:  Loss:     0.3007 Validation Accuracy: 0.655800\n",
      "Epoch 161, CIFAR-10 Batch 5:  Loss:     0.2495 Validation Accuracy: 0.657800\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss:     0.2916 Validation Accuracy: 0.657200\n",
      "Epoch 162, CIFAR-10 Batch 2:  Loss:     0.2816 Validation Accuracy: 0.661600\n",
      "Epoch 162, CIFAR-10 Batch 3:  Loss:     0.2709 Validation Accuracy: 0.664200\n",
      "Epoch 162, CIFAR-10 Batch 4:  Loss:     0.2878 Validation Accuracy: 0.656800\n",
      "Epoch 162, CIFAR-10 Batch 5:  Loss:     0.2650 Validation Accuracy: 0.659600\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss:     0.2773 Validation Accuracy: 0.664800\n",
      "Epoch 163, CIFAR-10 Batch 2:  Loss:     0.2741 Validation Accuracy: 0.653800\n",
      "Epoch 163, CIFAR-10 Batch 3:  Loss:     0.2633 Validation Accuracy: 0.661400\n",
      "Epoch 163, CIFAR-10 Batch 4:  Loss:     0.2643 Validation Accuracy: 0.663800\n",
      "Epoch 163, CIFAR-10 Batch 5:  Loss:     0.2576 Validation Accuracy: 0.653800\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss:     0.2784 Validation Accuracy: 0.656400\n",
      "Epoch 164, CIFAR-10 Batch 2:  Loss:     0.2717 Validation Accuracy: 0.657400\n",
      "Epoch 164, CIFAR-10 Batch 3:  Loss:     0.2684 Validation Accuracy: 0.661400\n",
      "Epoch 164, CIFAR-10 Batch 4:  Loss:     0.2840 Validation Accuracy: 0.654800\n",
      "Epoch 164, CIFAR-10 Batch 5:  Loss:     0.2565 Validation Accuracy: 0.656600\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss:     0.2824 Validation Accuracy: 0.655000\n",
      "Epoch 165, CIFAR-10 Batch 2:  Loss:     0.2700 Validation Accuracy: 0.662200\n",
      "Epoch 165, CIFAR-10 Batch 3:  Loss:     0.2710 Validation Accuracy: 0.655400\n",
      "Epoch 165, CIFAR-10 Batch 4:  Loss:     0.2846 Validation Accuracy: 0.651200\n",
      "Epoch 165, CIFAR-10 Batch 5:  Loss:     0.2756 Validation Accuracy: 0.642400\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss:     0.2771 Validation Accuracy: 0.655600\n",
      "Epoch 166, CIFAR-10 Batch 2:  Loss:     0.2757 Validation Accuracy: 0.650200\n",
      "Epoch 166, CIFAR-10 Batch 3:  Loss:     0.2916 Validation Accuracy: 0.648400\n",
      "Epoch 166, CIFAR-10 Batch 4:  Loss:     0.2850 Validation Accuracy: 0.658200\n",
      "Epoch 166, CIFAR-10 Batch 5:  Loss:     0.2529 Validation Accuracy: 0.649400\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss:     0.2903 Validation Accuracy: 0.654600\n",
      "Epoch 167, CIFAR-10 Batch 2:  Loss:     0.2792 Validation Accuracy: 0.651600\n",
      "Epoch 167, CIFAR-10 Batch 3:  Loss:     0.2687 Validation Accuracy: 0.646400\n",
      "Epoch 167, CIFAR-10 Batch 4:  Loss:     0.2822 Validation Accuracy: 0.657400\n",
      "Epoch 167, CIFAR-10 Batch 5:  Loss:     0.2608 Validation Accuracy: 0.641200\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss:     0.3125 Validation Accuracy: 0.648600\n",
      "Epoch 168, CIFAR-10 Batch 2:  Loss:     0.3111 Validation Accuracy: 0.650800\n",
      "Epoch 168, CIFAR-10 Batch 3:  Loss:     0.2731 Validation Accuracy: 0.646400\n",
      "Epoch 168, CIFAR-10 Batch 4:  Loss:     0.2887 Validation Accuracy: 0.653400\n",
      "Epoch 168, CIFAR-10 Batch 5:  Loss:     0.2486 Validation Accuracy: 0.654000\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss:     0.2920 Validation Accuracy: 0.663800\n",
      "Epoch 169, CIFAR-10 Batch 2:  Loss:     0.3040 Validation Accuracy: 0.647800\n",
      "Epoch 169, CIFAR-10 Batch 3:  Loss:     0.2784 Validation Accuracy: 0.653800\n",
      "Epoch 169, CIFAR-10 Batch 4:  Loss:     0.2853 Validation Accuracy: 0.651200\n",
      "Epoch 169, CIFAR-10 Batch 5:  Loss:     0.2632 Validation Accuracy: 0.648000\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss:     0.2806 Validation Accuracy: 0.663200\n",
      "Epoch 170, CIFAR-10 Batch 2:  Loss:     0.2779 Validation Accuracy: 0.657000\n",
      "Epoch 170, CIFAR-10 Batch 3:  Loss:     0.2576 Validation Accuracy: 0.651200\n",
      "Epoch 170, CIFAR-10 Batch 4:  Loss:     0.2652 Validation Accuracy: 0.657800\n",
      "Epoch 170, CIFAR-10 Batch 5:  Loss:     0.2507 Validation Accuracy: 0.647800\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss:     0.2754 Validation Accuracy: 0.659000\n",
      "Epoch 171, CIFAR-10 Batch 2:  Loss:     0.2734 Validation Accuracy: 0.652600\n",
      "Epoch 171, CIFAR-10 Batch 3:  Loss:     0.2548 Validation Accuracy: 0.652800\n",
      "Epoch 171, CIFAR-10 Batch 4:  Loss:     0.2670 Validation Accuracy: 0.647200\n",
      "Epoch 171, CIFAR-10 Batch 5:  Loss:     0.2374 Validation Accuracy: 0.656800\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss:     0.2782 Validation Accuracy: 0.662200\n",
      "Epoch 172, CIFAR-10 Batch 2:  Loss:     0.2797 Validation Accuracy: 0.656400\n",
      "Epoch 172, CIFAR-10 Batch 3:  Loss:     0.2586 Validation Accuracy: 0.651400\n",
      "Epoch 172, CIFAR-10 Batch 4:  Loss:     0.2777 Validation Accuracy: 0.656400\n",
      "Epoch 172, CIFAR-10 Batch 5:  Loss:     0.2466 Validation Accuracy: 0.654000\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss:     0.2664 Validation Accuracy: 0.656400\n",
      "Epoch 173, CIFAR-10 Batch 2:  Loss:     0.2764 Validation Accuracy: 0.658200\n",
      "Epoch 173, CIFAR-10 Batch 3:  Loss:     0.2591 Validation Accuracy: 0.652400\n",
      "Epoch 173, CIFAR-10 Batch 4:  Loss:     0.2751 Validation Accuracy: 0.653800\n",
      "Epoch 173, CIFAR-10 Batch 5:  Loss:     0.2595 Validation Accuracy: 0.645200\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss:     0.2822 Validation Accuracy: 0.664000\n",
      "Epoch 174, CIFAR-10 Batch 2:  Loss:     0.2722 Validation Accuracy: 0.648600\n",
      "Epoch 174, CIFAR-10 Batch 3:  Loss:     0.2525 Validation Accuracy: 0.653200\n",
      "Epoch 174, CIFAR-10 Batch 4:  Loss:     0.2798 Validation Accuracy: 0.643600\n",
      "Epoch 174, CIFAR-10 Batch 5:  Loss:     0.2660 Validation Accuracy: 0.646400\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss:     0.2846 Validation Accuracy: 0.660200\n",
      "Epoch 175, CIFAR-10 Batch 2:  Loss:     0.2792 Validation Accuracy: 0.650000\n",
      "Epoch 175, CIFAR-10 Batch 3:  Loss:     0.2920 Validation Accuracy: 0.648200\n",
      "Epoch 175, CIFAR-10 Batch 4:  Loss:     0.2711 Validation Accuracy: 0.649200\n",
      "Epoch 175, CIFAR-10 Batch 5:  Loss:     0.2591 Validation Accuracy: 0.652400\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss:     0.2773 Validation Accuracy: 0.659600\n",
      "Epoch 176, CIFAR-10 Batch 2:  Loss:     0.2746 Validation Accuracy: 0.650400\n",
      "Epoch 176, CIFAR-10 Batch 3:  Loss:     0.2537 Validation Accuracy: 0.655000\n",
      "Epoch 176, CIFAR-10 Batch 4:  Loss:     0.2547 Validation Accuracy: 0.653000\n",
      "Epoch 176, CIFAR-10 Batch 5:  Loss:     0.2439 Validation Accuracy: 0.657400\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss:     0.2739 Validation Accuracy: 0.663600\n",
      "Epoch 177, CIFAR-10 Batch 2:  Loss:     0.2640 Validation Accuracy: 0.649800\n",
      "Epoch 177, CIFAR-10 Batch 3:  Loss:     0.2496 Validation Accuracy: 0.651800\n",
      "Epoch 177, CIFAR-10 Batch 4:  Loss:     0.2733 Validation Accuracy: 0.654200\n",
      "Epoch 177, CIFAR-10 Batch 5:  Loss:     0.2501 Validation Accuracy: 0.656200\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss:     0.2839 Validation Accuracy: 0.656000\n",
      "Epoch 178, CIFAR-10 Batch 2:  Loss:     0.2766 Validation Accuracy: 0.649600\n",
      "Epoch 178, CIFAR-10 Batch 3:  Loss:     0.2713 Validation Accuracy: 0.652600\n",
      "Epoch 178, CIFAR-10 Batch 4:  Loss:     0.2556 Validation Accuracy: 0.652600\n",
      "Epoch 178, CIFAR-10 Batch 5:  Loss:     0.2401 Validation Accuracy: 0.664000\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss:     0.2575 Validation Accuracy: 0.660000\n",
      "Epoch 179, CIFAR-10 Batch 2:  Loss:     0.2668 Validation Accuracy: 0.654600\n",
      "Epoch 179, CIFAR-10 Batch 3:  Loss:     0.2639 Validation Accuracy: 0.644800\n",
      "Epoch 179, CIFAR-10 Batch 4:  Loss:     0.2841 Validation Accuracy: 0.644200\n",
      "Epoch 179, CIFAR-10 Batch 5:  Loss:     0.2533 Validation Accuracy: 0.657400\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss:     0.2673 Validation Accuracy: 0.659800\n",
      "Epoch 180, CIFAR-10 Batch 2:  Loss:     0.2615 Validation Accuracy: 0.650000\n",
      "Epoch 180, CIFAR-10 Batch 3:  Loss:     0.2831 Validation Accuracy: 0.639400\n",
      "Epoch 180, CIFAR-10 Batch 4:  Loss:     0.2669 Validation Accuracy: 0.645400\n",
      "Epoch 180, CIFAR-10 Batch 5:  Loss:     0.2525 Validation Accuracy: 0.655200\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss:     0.2660 Validation Accuracy: 0.664000\n",
      "Epoch 181, CIFAR-10 Batch 2:  Loss:     0.2729 Validation Accuracy: 0.651400\n",
      "Epoch 181, CIFAR-10 Batch 3:  Loss:     0.2653 Validation Accuracy: 0.647200\n",
      "Epoch 181, CIFAR-10 Batch 4:  Loss:     0.2845 Validation Accuracy: 0.635400\n",
      "Epoch 181, CIFAR-10 Batch 5:  Loss:     0.2547 Validation Accuracy: 0.651600\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss:     0.2564 Validation Accuracy: 0.664400\n",
      "Epoch 182, CIFAR-10 Batch 2:  Loss:     0.2618 Validation Accuracy: 0.653200\n",
      "Epoch 182, CIFAR-10 Batch 3:  Loss:     0.2640 Validation Accuracy: 0.647800\n",
      "Epoch 182, CIFAR-10 Batch 4:  Loss:     0.2528 Validation Accuracy: 0.650200\n",
      "Epoch 182, CIFAR-10 Batch 5:  Loss:     0.2409 Validation Accuracy: 0.664800\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss:     0.2545 Validation Accuracy: 0.658400\n",
      "Epoch 183, CIFAR-10 Batch 2:  Loss:     0.2348 Validation Accuracy: 0.658600\n",
      "Epoch 183, CIFAR-10 Batch 3:  Loss:     0.2419 Validation Accuracy: 0.648400\n",
      "Epoch 183, CIFAR-10 Batch 4:  Loss:     0.2743 Validation Accuracy: 0.637000\n",
      "Epoch 183, CIFAR-10 Batch 5:  Loss:     0.2417 Validation Accuracy: 0.655600\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss:     0.2503 Validation Accuracy: 0.656400\n",
      "Epoch 184, CIFAR-10 Batch 2:  Loss:     0.2445 Validation Accuracy: 0.657000\n",
      "Epoch 184, CIFAR-10 Batch 3:  Loss:     0.2419 Validation Accuracy: 0.643800\n",
      "Epoch 184, CIFAR-10 Batch 4:  Loss:     0.2554 Validation Accuracy: 0.652400\n",
      "Epoch 184, CIFAR-10 Batch 5:  Loss:     0.2443 Validation Accuracy: 0.657600\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss:     0.2504 Validation Accuracy: 0.655400\n",
      "Epoch 185, CIFAR-10 Batch 2:  Loss:     0.2356 Validation Accuracy: 0.662200\n",
      "Epoch 185, CIFAR-10 Batch 3:  Loss:     0.2453 Validation Accuracy: 0.650200\n",
      "Epoch 185, CIFAR-10 Batch 4:  Loss:     0.2685 Validation Accuracy: 0.642200\n",
      "Epoch 185, CIFAR-10 Batch 5:  Loss:     0.2438 Validation Accuracy: 0.658400\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss:     0.2511 Validation Accuracy: 0.661800\n",
      "Epoch 186, CIFAR-10 Batch 2:  Loss:     0.2388 Validation Accuracy: 0.647000\n",
      "Epoch 186, CIFAR-10 Batch 3:  Loss:     0.2524 Validation Accuracy: 0.642200\n",
      "Epoch 186, CIFAR-10 Batch 4:  Loss:     0.2685 Validation Accuracy: 0.634000\n",
      "Epoch 186, CIFAR-10 Batch 5:  Loss:     0.2668 Validation Accuracy: 0.656000\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss:     0.2619 Validation Accuracy: 0.656400\n",
      "Epoch 187, CIFAR-10 Batch 2:  Loss:     0.2312 Validation Accuracy: 0.649200\n",
      "Epoch 187, CIFAR-10 Batch 3:  Loss:     0.2523 Validation Accuracy: 0.638600\n",
      "Epoch 187, CIFAR-10 Batch 4:  Loss:     0.2534 Validation Accuracy: 0.639400\n",
      "Epoch 187, CIFAR-10 Batch 5:  Loss:     0.2589 Validation Accuracy: 0.652400\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss:     0.2405 Validation Accuracy: 0.660200\n",
      "Epoch 188, CIFAR-10 Batch 2:  Loss:     0.2369 Validation Accuracy: 0.656400\n",
      "Epoch 188, CIFAR-10 Batch 3:  Loss:     0.2548 Validation Accuracy: 0.649000\n",
      "Epoch 188, CIFAR-10 Batch 4:  Loss:     0.2680 Validation Accuracy: 0.639600\n",
      "Epoch 188, CIFAR-10 Batch 5:  Loss:     0.2562 Validation Accuracy: 0.655200\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss:     0.2552 Validation Accuracy: 0.663400\n",
      "Epoch 189, CIFAR-10 Batch 2:  Loss:     0.2317 Validation Accuracy: 0.655000\n",
      "Epoch 189, CIFAR-10 Batch 3:  Loss:     0.2294 Validation Accuracy: 0.655200\n",
      "Epoch 189, CIFAR-10 Batch 4:  Loss:     0.2602 Validation Accuracy: 0.642800\n",
      "Epoch 189, CIFAR-10 Batch 5:  Loss:     0.2495 Validation Accuracy: 0.657600\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss:     0.2493 Validation Accuracy: 0.662200\n",
      "Epoch 190, CIFAR-10 Batch 2:  Loss:     0.2475 Validation Accuracy: 0.654000\n",
      "Epoch 190, CIFAR-10 Batch 3:  Loss:     0.2247 Validation Accuracy: 0.647400\n",
      "Epoch 190, CIFAR-10 Batch 4:  Loss:     0.2451 Validation Accuracy: 0.654400\n",
      "Epoch 190, CIFAR-10 Batch 5:  Loss:     0.2352 Validation Accuracy: 0.650800\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss:     0.2439 Validation Accuracy: 0.665600\n",
      "Epoch 191, CIFAR-10 Batch 2:  Loss:     0.2564 Validation Accuracy: 0.649000\n",
      "Epoch 191, CIFAR-10 Batch 3:  Loss:     0.2374 Validation Accuracy: 0.646000\n",
      "Epoch 191, CIFAR-10 Batch 4:  Loss:     0.2547 Validation Accuracy: 0.635200\n",
      "Epoch 191, CIFAR-10 Batch 5:  Loss:     0.2494 Validation Accuracy: 0.653000\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss:     0.2396 Validation Accuracy: 0.667200\n",
      "Epoch 192, CIFAR-10 Batch 2:  Loss:     0.2437 Validation Accuracy: 0.644800\n",
      "Epoch 192, CIFAR-10 Batch 3:  Loss:     0.2351 Validation Accuracy: 0.654600\n",
      "Epoch 192, CIFAR-10 Batch 4:  Loss:     0.2496 Validation Accuracy: 0.648800\n",
      "Epoch 192, CIFAR-10 Batch 5:  Loss:     0.2266 Validation Accuracy: 0.653200\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss:     0.2444 Validation Accuracy: 0.665400\n",
      "Epoch 193, CIFAR-10 Batch 2:  Loss:     0.2380 Validation Accuracy: 0.649600\n",
      "Epoch 193, CIFAR-10 Batch 3:  Loss:     0.2340 Validation Accuracy: 0.639600\n",
      "Epoch 193, CIFAR-10 Batch 4:  Loss:     0.2390 Validation Accuracy: 0.647600\n",
      "Epoch 193, CIFAR-10 Batch 5:  Loss:     0.2305 Validation Accuracy: 0.660000\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss:     0.2512 Validation Accuracy: 0.661200\n",
      "Epoch 194, CIFAR-10 Batch 2:  Loss:     0.2303 Validation Accuracy: 0.651800\n",
      "Epoch 194, CIFAR-10 Batch 3:  Loss:     0.2211 Validation Accuracy: 0.653000\n",
      "Epoch 194, CIFAR-10 Batch 4:  Loss:     0.2355 Validation Accuracy: 0.654200\n",
      "Epoch 194, CIFAR-10 Batch 5:  Loss:     0.2332 Validation Accuracy: 0.651600\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss:     0.2443 Validation Accuracy: 0.669000\n",
      "Epoch 195, CIFAR-10 Batch 2:  Loss:     0.2209 Validation Accuracy: 0.655200\n",
      "Epoch 195, CIFAR-10 Batch 3:  Loss:     0.2146 Validation Accuracy: 0.648400\n",
      "Epoch 195, CIFAR-10 Batch 4:  Loss:     0.2253 Validation Accuracy: 0.648600\n",
      "Epoch 195, CIFAR-10 Batch 5:  Loss:     0.2175 Validation Accuracy: 0.661000\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss:     0.2310 Validation Accuracy: 0.667400\n",
      "Epoch 196, CIFAR-10 Batch 2:  Loss:     0.2333 Validation Accuracy: 0.652000\n",
      "Epoch 196, CIFAR-10 Batch 3:  Loss:     0.2268 Validation Accuracy: 0.652000\n",
      "Epoch 196, CIFAR-10 Batch 4:  Loss:     0.2508 Validation Accuracy: 0.651400\n",
      "Epoch 196, CIFAR-10 Batch 5:  Loss:     0.2464 Validation Accuracy: 0.653000\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss:     0.2470 Validation Accuracy: 0.662800\n",
      "Epoch 197, CIFAR-10 Batch 2:  Loss:     0.2283 Validation Accuracy: 0.648200\n",
      "Epoch 197, CIFAR-10 Batch 3:  Loss:     0.2263 Validation Accuracy: 0.649200\n",
      "Epoch 197, CIFAR-10 Batch 4:  Loss:     0.2349 Validation Accuracy: 0.642400\n",
      "Epoch 197, CIFAR-10 Batch 5:  Loss:     0.2262 Validation Accuracy: 0.655800\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss:     0.2451 Validation Accuracy: 0.669200\n",
      "Epoch 198, CIFAR-10 Batch 2:  Loss:     0.2457 Validation Accuracy: 0.643000\n",
      "Epoch 198, CIFAR-10 Batch 3:  Loss:     0.2130 Validation Accuracy: 0.648400\n",
      "Epoch 198, CIFAR-10 Batch 4:  Loss:     0.2361 Validation Accuracy: 0.649200\n",
      "Epoch 198, CIFAR-10 Batch 5:  Loss:     0.2296 Validation Accuracy: 0.656600\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss:     0.2546 Validation Accuracy: 0.655200\n",
      "Epoch 199, CIFAR-10 Batch 2:  Loss:     0.2391 Validation Accuracy: 0.658600\n",
      "Epoch 199, CIFAR-10 Batch 3:  Loss:     0.2183 Validation Accuracy: 0.652800\n",
      "Epoch 199, CIFAR-10 Batch 4:  Loss:     0.2321 Validation Accuracy: 0.645000\n",
      "Epoch 199, CIFAR-10 Batch 5:  Loss:     0.2110 Validation Accuracy: 0.662200\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss:     0.2366 Validation Accuracy: 0.660600\n",
      "Epoch 200, CIFAR-10 Batch 2:  Loss:     0.2249 Validation Accuracy: 0.655600\n",
      "Epoch 200, CIFAR-10 Batch 3:  Loss:     0.2151 Validation Accuracy: 0.650400\n",
      "Epoch 200, CIFAR-10 Batch 4:  Loss:     0.2398 Validation Accuracy: 0.641800\n",
      "Epoch 200, CIFAR-10 Batch 5:  Loss:     0.2110 Validation Accuracy: 0.659800\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.6505141973495483\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJsciIiIiItH/B/tp606fRy6UAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3b334f43c8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-70% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 70%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
